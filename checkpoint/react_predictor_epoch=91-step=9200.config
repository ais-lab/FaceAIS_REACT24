# ACRCHITECTURE CONFIG
vocab_size=tokenizer_config.quantize_codebook_size,  # this is the vocab size of the face token but in predictor
vocab_face_size=tokenizer_config.quantize_codebook_size,  # this the vocab size of the face token
vocab_sound_size=320,  # if use wav2vec2 feature, the sound vocab size is not needed
is_twisted=False,  # Tested: twist and no twist make little difference
block_size=256,  # this is context length, pad if not enough, cut if too long, tried 128, 256
n_embd=360,  # TODO: increase the size of the model
n_head=12,
n_layer=8,
sound_factor=2,  # the factor to downsample the sound token to match the face token
use_wav2vec2_feature=True,  # use wav2vec2 vector instead of vq-wav2vec make the model's output more synchronized with sound
patch_size=32,  # should be any factor of block_size of tokenizer?
# TRAINING CONFIG
learning_rate=0.001,  # this learningrate produce best result
warmup_iters=200,  # should be 2~5% of the total iteration
min_lr=6e-4,
past_mask_p=0.5,  # 30% of the time we mask a random number of past token
dropout=0.1,  # NanoGPT suggest no dropout, but it seems overfit if no dropout
attn_dropout=0.1,
embd_pdrop=0.1,
resid_dropout=0.1,
bias=True,
# TEST CONFIG
sampling_top_k=512,
sampling_temperature=1,  # the higher the temperature, the more random the output
test_extend_factor=10,
only_speaker=False,
collect_metrics_in="val",
render_skip_step=1,