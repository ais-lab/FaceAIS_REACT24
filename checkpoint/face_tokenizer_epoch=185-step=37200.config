input_dim=58,
output_dim=58,
emotion_output_dim=25,
block_size=32,
n_embd=252,
n_head=12,
n_layer=12,
quantize_type="fsq",  # lfq, vq, fsq
quantize_codebook_size=2048,  # the vocab size: 512, 1024, 2048
quantize_levels=[8, 5, 5, 5]